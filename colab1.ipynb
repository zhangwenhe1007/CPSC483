{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangwenhe1007/CPSC483/blob/main/colab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R0-JXHoqhtD"
      },
      "source": [
        "## Outline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETQPVYpoqsvY"
      },
      "source": [
        "- Basic operation\n",
        "- Models\n",
        "- Datasets\n",
        "- Training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBRS0TB5mo7o"
      },
      "source": [
        "## Basic operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dtrab3iOSzgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f371f9e-411b-4821-c1b1-f12146e0f660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.5.1+cu124\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# import the pytorch library into environment and check its version\n",
        "import torch\n",
        "print(\"Using torch\", torch.__version__)\n",
        "\n",
        "# check if GPU is available and detectable. cpu is ok for this homework.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei5_O8ZXtlGb"
      },
      "source": [
        "### Create Tensor\n",
        "\n",
        "Tensor are the central data abstraction in PyTorch. You can generalize it to the concept you already know. For example, a vector is a 1-D tensor, and a matrix is a 2-D tensor. We can easily create a tensor of various shapes and number of dimensions by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KR0ln6OxSzgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e780d0-ba22-4546-a02c-942f5cd110d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ],
      "source": [
        "# construct a 1-D tensor from a list\n",
        "x = torch.tensor([1, 2, 3])\n",
        "print(x)\n",
        "\n",
        "# construct a 2-D tensor from a list\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCY73L2LGoS8"
      },
      "source": [
        "Given a tensor, we can obtain its shape by using `.size` or `.shape` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HMg1NUP9Gnkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db8ccc3-d61b-45b6-ef9c-ac93296027e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x is torch.Size([2, 3])\n",
            "Size of x is torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of x is\", x.shape)\n",
        "\n",
        "print(\"Size of x is\", x.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKHOYbMCE9b7"
      },
      "source": [
        "Here are alternatives to create a tensor:\n",
        "\n",
        "* `torch.zeros`: Creates a tensor filled with zeros\n",
        "* `torch.ones`: Creates a tensor filled with ones\n",
        "* `torch.rand`: Creates a tensor with random values uniformly sampled between 0 and 1\n",
        "* `torch.randn`: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1\n",
        "* `torch.arange`: Creates a 1-D tensor containing the values $N,N+1,N+2,...,M$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RfZq2L2uEphL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a9f548-82f4-4ba1-abf2-46c10182b0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor created by torch.rand function tensor([[0.6970, 0.7008, 0.0621, 0.6411, 0.0224],\n",
            "        [0.2557, 0.8613, 0.2156, 0.8299, 0.4355],\n",
            "        [0.4162, 0.0901, 0.3313, 0.0306, 0.0776],\n",
            "        [0.9155, 0.2569, 0.4812, 0.7076, 0.4515]])\n",
            "Tensor created by torch.arange function tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "# construct a tensor with random numbers with shape (4, 5)\n",
        "x = torch.rand(4, 5)\n",
        "print(\"Tensor created by torch.rand function\", x)\n",
        "\n",
        "# construct a tensor filled with the scalar value 0 with shape (2, 3, 4)\n",
        "x = torch.zeros(2, 3, 4)\n",
        "print(\"Tensor created by torch.arange function\", x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc9hyQTnIDX3"
      },
      "source": [
        "### Tensor Operations\n",
        "\n",
        "PyTorch supports various tensor operations. A full list of operations can be found in the [PyTorch documentation](https://pytorch.org/docs/stable/tensors.html#). For example, we can add two tensors with same shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8XSh1c4CEpst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a493f5-275c-440c-99f4-bdfb8a775722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1: tensor([[0.2344, 0.2836],\n",
            "        [0.0476, 0.9778]])\n",
            "x2: tensor([[0.3047, 0.4843],\n",
            "        [0.0569, 0.0896]])\n",
            "x3: tensor([[0.5391, 0.7679],\n",
            "        [0.1045, 1.0675]])\n"
          ]
        }
      ],
      "source": [
        "x1 = torch.rand(2, 2)\n",
        "x2 = torch.rand(2, 2)\n",
        "x3 = x1 + x2  # return the sum of x1 and x2\n",
        "\n",
        "print(\"x1:\", x1)\n",
        "print(\"x2:\", x2)\n",
        "print(\"x3:\", x3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMKyR-QoJiYK"
      },
      "source": [
        "We also can calculate the matrix product between two tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Enw4SKbvKKwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e424f1df-3f33-4571-b493-70ed78cb6fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of x: torch.Size([3]) x: tensor([0.9537, 0.9856, 0.2353])\n",
            "shape of W: torch.Size([2, 3]) W: tensor([[0.0845, 0.7125, 0.5241],\n",
            "        [0.5185, 0.9264, 0.4247]])\n",
            "shape of h: torch.Size([2]) h: tensor([0.9062, 1.5075])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(3)\n",
        "W = torch.rand(2, 3)\n",
        "h = torch.matmul(W, x)  # return the product between W and x\n",
        "\n",
        "print(\"shape of x:\", x.shape, \"x:\", x)  # shape: [3]\n",
        "print(\"shape of W:\", W.shape, \"W:\", W)  # shape: [2, 3]\n",
        "print(\"shape of h:\", h.shape, \"h:\", h)  # shape: [2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WOoJMhaLpZU"
      },
      "source": [
        "Another common operation aims at changing the shape of a tensor. A tensor'size can be re-organized to any other shape with the same number of elements. In PyTorch, this operation is called `view` or `reshape`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mWT7phb4KK1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6d304d-79fb-44eb-a8fb-994f972c9215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of x: torch.Size([2, 3]) x: tensor([[0.0989, 0.6164, 0.1402],\n",
            "        [0.4111, 0.7254, 0.7742]])\n",
            "shape of x1: torch.Size([6]) x1: tensor([0.0989, 0.6164, 0.1402, 0.4111, 0.7254, 0.7742])\n",
            "shape of x2: torch.Size([3, 2]) x2: tensor([[0.0989, 0.6164],\n",
            "        [0.1402, 0.4111],\n",
            "        [0.7254, 0.7742]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(2, 3)\n",
        "x1 = x.view(6)\n",
        "x2 = x.reshape(3, 2)\n",
        "\n",
        "print(\"shape of x:\", x.shape, \"x:\", x)  # shape: [2, 3]\n",
        "print(\"shape of x1:\", x1.shape, \"x1:\", x1)  # shape: [6]\n",
        "print(\"shape of x2:\", x2.shape, \"x2:\", x2)  # shape: [3, 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddiMjUdbmW24"
      },
      "source": [
        "### Question 1 (5 points)\n",
        "\n",
        "Given a 1-D tensor, what is the index of its maximum value?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Wg17zDRQo2HG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892477cb-22bd-4f19-ab93-d2ff6d6b0521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index of the maximum value of tensor([0.7087, 0.8586, 0.4048, 0.4372, 0.6913]) is 1\n"
          ]
        }
      ],
      "source": [
        "def get_idx_max(x):\n",
        "  # TODO: Implement a function that takes a tensor object\n",
        "  # and returns the index of its maximum value.\n",
        "\n",
        "  max_idx = -1\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  max_idx = torch.argmax(x)\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return max_idx\n",
        "\n",
        "x = torch.rand(5)\n",
        "max_idx = get_idx_max(x)\n",
        "print('index of the maximum value of {} is {}'.format(x, max_idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-DjqAslNX29"
      },
      "source": [
        "## Models\n",
        "\n",
        "We can use PyTorch to build deep learning model. Here we will give an example of using multi-layer perceptron (MLP) to perform image classification. We'll start with building a MLP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6qFGvRYJSErW"
      },
      "outputs": [],
      "source": [
        "# import neural network module of PyTorch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYsSkgCxSGEW"
      },
      "source": [
        "A MLP is built by stacking multiple linear layers:\n",
        "\n",
        "$$Linear(X):=A X + b$$\n",
        "\n",
        "with $X\\in \\mathbb{R}^{n\\times k}$, $A\\in \\mathbb{R}^{m\\times n}$, $b\\in\\mathbb{R}^{m\\times 1}$. We can implement a linear layer with `nn.Linear`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AWhLXWFWVMQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7654c05-56e6-4719-c939-b018bb0ed7e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "linear = nn.Linear(5, 10)  # creat a linear layer with n=5, m=10\n",
        "\n",
        "print(linear.weight.shape)  # [10, 5]\n",
        "print(linear.bias.shape)  # [10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePjdm2WKb0rM"
      },
      "source": [
        "We can feed a tensor into the linear module to perform linear transformation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Kcb3PWpNcAdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabe89fb-f512-45ce-ca82-26f5b729afb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([[0.9183, 0.0668, 0.5090, 0.2909, 0.2433],\n",
            "        [0.3790, 0.7998, 0.5982, 0.7290, 0.5050],\n",
            "        [0.9789, 0.2055, 0.3642, 0.3259, 0.0725],\n",
            "        [0.0163, 0.0303, 0.8101, 0.0119, 0.8651],\n",
            "        [0.0535, 0.0426, 0.4314, 0.9897, 0.2439],\n",
            "        [0.4354, 0.2903, 0.4194, 0.7327, 0.3084]])\n",
            "Linear(x): tensor([[-0.0240, -0.0502,  0.0014, -0.7322,  0.4604,  0.0738, -0.4235,  0.8905,\n",
            "         -0.1805, -0.2601],\n",
            "        [-0.1510,  0.2231, -0.2231, -0.8542,  0.6777, -0.4660,  0.0677,  1.0678,\n",
            "          0.1046, -0.3007],\n",
            "        [-0.0673,  0.0023, -0.0235, -0.6445,  0.4480,  0.1996, -0.4417,  0.8955,\n",
            "         -0.1476, -0.2144],\n",
            "        [-0.0956, -0.0925,  0.0567, -0.8936,  0.4634, -0.5500, -0.1248,  0.7129,\n",
            "          0.2492, -0.4474],\n",
            "        [-0.4346, -0.1043, -0.1992, -0.7918,  0.5377, -0.5242, -0.0341,  0.5632,\n",
            "         -0.0961, -0.4043],\n",
            "        [-0.2222,  0.0511, -0.1876, -0.7775,  0.5537, -0.2892, -0.1275,  0.8068,\n",
            "         -0.0728, -0.3276]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(6, 5)\n",
        "\n",
        "print(\"x:\", x)\n",
        "print(\"Linear(x):\", linear(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2RN7fOwV0pk"
      },
      "source": [
        "Besides, to expand the capability and allow model to learn non-linear transformation between inputs and outputs, we will introduce the non-linear activation function between two linear layers. Here we use ReLU activation function:\n",
        "\n",
        "![](https://github.com/Graph-and-Geometric-Learning/CPSC483-colab/blob/main/fig/relu.png?raw=1)\n",
        "\n",
        "We can implement a ReLU functino by using `F.relu` or `nn.ReLU()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "56ygKD1DZajR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d09b58-6654-4220-8160-5582dc77621a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([[0.1472, 0.2249, 0.9148],\n",
            "        [0.0834, 0.4894, 0.0737]])\n",
            "F.relu(x): tensor([[0.1472, 0.2249, 0.9148],\n",
            "        [0.0834, 0.4894, 0.0737]])\n",
            "nn.relu(x): tensor([[0.1472, 0.2249, 0.9148],\n",
            "        [0.0834, 0.4894, 0.0737]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(2, 3)\n",
        "act_fn = nn.ReLU()\n",
        "\n",
        "print(\"x:\", x)\n",
        "print(\"F.relu(x):\", F.relu(x))\n",
        "print(\"nn.relu(x):\", act_fn(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1ikBfJFZ1iy"
      },
      "source": [
        "### Question 2 (10 points)\n",
        "\n",
        "Now it is your time to build a MLP with linear layer and non-linear function. We use `nn.Module` to define a MLP class containing the basic modules of MLP. The modules we need will be defined in `__init__` function and the calculation will be performed in `forward` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_GIzPoJINXKm"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Define two Linear modules and a ReLU function.\n",
        "        # The input size and output size of first Linear module should be input_dim and hidden_dim\n",
        "        # The input size and output size of second Linear module should be hidden_dim and output_dim\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## (~3 line of code)\n",
        "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lin2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.act_relu = nn.ReLU()\n",
        "\n",
        "        #########################################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1)  # reshape the tensor to a 1-D vector\n",
        "\n",
        "        out = 0\n",
        "\n",
        "        # TODO: Use the modules you define in __init__ to perform calculation.\n",
        "        # ReLU function should be used in the middle of two Linear modules.\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## (~3 line of code)\n",
        "        hidden_out = self.lin1(x)\n",
        "        relu_out = self.act_relu(hidden_out)\n",
        "        out = self.lin2(relu_out)\n",
        "\n",
        "        #########################################\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOoSsST0uMQc"
      },
      "source": [
        "As shown in the `MLP` class, we initialize the modules we need in `__init__` function, and perform the calculation of the model to predict the results in `forward` function. Here is an example to instantiate a MLP model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ukjByiB1ztz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d637e6-7a8f-4235-ac83-91954c35b4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (lin1): Linear(in_features=10, out_features=20, bias=True)\n",
            "  (lin2): Linear(in_features=20, out_features=10, bias=True)\n",
            "  (act_relu): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = MLP(input_dim=10, hidden_dim=20, output_dim=10)\n",
        "print(model)  # show all the submodules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwodgWvlsmLT"
      },
      "source": [
        "We can obtain the parameters of a module by its `parameters` functions, or `named_parameters` to get a name to each parameter object. For our MLP, we have the following parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1O5O2Dc1slo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e628dc4-ab6d-4bbb-8ca2-88b727e65fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter lin1.weight, shape torch.Size([20, 10])\n",
            "Parameter lin1.bias, shape torch.Size([20])\n",
            "Parameter lin2.weight, shape torch.Size([10, 20])\n",
            "Parameter lin2.bias, shape torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter {name}, shape {param.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztrLP3j8vtaR"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "We are going to use a dataset for image classification called MNIST. It is a large database of handwritten digits, and widely used for training and testing in the field of image processing. We can esily load this dataset with the help of PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8gn1LsIryRDP"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0EimIP8yZJW"
      },
      "source": [
        "After importing required library, we can download the dataset of MNIST and save it in a folder. It will automatically create the folder if it doesn't exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "P9zunwfNyvK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588acdf8-8f1a-4187-88f6-0adafb9780cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to .data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 54.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .data/MNIST/raw/train-images-idx3-ubyte.gz to .data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to .data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 2.02MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .data/MNIST/raw/train-labels-idx1-ubyte.gz to .data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to .data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .data/MNIST/raw/t10k-images-idx3-ubyte.gz to .data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to .data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.06MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting .data/MNIST/raw/t10k-labels-idx1-ubyte.gz to .data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ROOT = '.data'  # the folder for saving dataset\n",
        "\n",
        "train_data = datasets.MNIST(root=ROOT,\n",
        "                            train=True,\n",
        "                            download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Z8UVv_y40u"
      },
      "source": [
        "To improve the model's generalization and robustness, we usually will preprocess and augment our dataset. Data augmentation is a group of methods for creating new data points from existing data in order to increase the amount of data. These operations are included in `transforms.Compose` function. Here are some common transforms:\n",
        "\n",
        "- `RandomRotation` - randomly rotates the image between `(-x, +x)` degrees, where we have set `x = 5`.\n",
        "- `RandomCrop` - randomly taking a square crop of the image.\n",
        "- `ToTensor()` - this converts the image from a PIL format into a PyTorch tensor.\n",
        "- `Normalize` - this subtracts the mean and divides by the standard deviations given.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-23N4YwR2b4y"
      },
      "outputs": [],
      "source": [
        "mean = train_data.data.float().mean() / 255\n",
        "std = train_data.data.float().std() / 255\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                            transforms.RandomRotation(5, fill=(0,)),\n",
        "                            transforms.RandomCrop(28, padding=2),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize(mean=[mean], std=[std])\n",
        "                                      ])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean=[mean], std=[std])\n",
        "                                     ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmZBnLbt2nWc"
      },
      "source": [
        "Now with the transform function, we can load the train and test dataset of MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iBgm1GKn2jLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9b3052-cdf6-4888-81aa-076466f559cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 50000\n",
            "Number of training examples: 10000\n",
            "Number of testing examples: 10000\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.MNIST(root=ROOT,\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=train_transforms)\n",
        "\n",
        "test_data = datasets.MNIST(root=ROOT,\n",
        "                           train=False,\n",
        "                           download=True,\n",
        "                           transform=test_transforms)\n",
        "\n",
        "\n",
        "\n",
        "# Define the size of your training and validation datasets\n",
        "train_size = 50000  # 50000 for training\n",
        "val_size = len(train_data) - train_size  # 10000 for validation\n",
        "\n",
        "# Split the dataset\n",
        "train_data, val_data = data.random_split(train_data, [train_size, val_size])\n",
        "\n",
        "print(f'Number of training examples: {len(train_data)}')  # check the number of the image in the train datasets\n",
        "print(f'Number of training examples: {len(val_data)}')  # check the number of the image in the validation datasets\n",
        "print(f'Number of testing examples: {len(test_data)}')  # check the number of the image in the test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja4x5h5EZZ_4"
      },
      "source": [
        "**NOTE**: The train/validation split is not ideal here because the validation dataset applies the `train_transform`, which is not what we want. However, we have kept it this way for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_train_classes = len(torch.unique(train_data.dataset.targets[train_data.indices]))\n",
        "print(f'Number of classes: {num_train_classes}')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.imshow(train_data[100][0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "p_BRfA-qv6eW",
        "outputId": "7a27c738-7722-43df-84d1-df0652425f99"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7cd2529b0210>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEvPG5hIfuU5"
      },
      "source": [
        "### Question 3 (5 points)\n",
        "\n",
        "What is the label of the image with index 100 in the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VDs-UuOlf4md",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4eb81f7-2c63-4377-c60d-739393ec017a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image with index 100 has label 0\n"
          ]
        }
      ],
      "source": [
        "def get_image_label(dataset, idx):\n",
        "  # TODO: Implement a function that takes a dataset object,\n",
        "  # an index of a image within the dataset, and returns the class/label\n",
        "  # of the image (as an integer).\n",
        "\n",
        "  label = -1\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  label = dataset[idx][1]\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return label\n",
        "\n",
        "idx = 100\n",
        "label = get_image_label(train_data, idx)\n",
        "print('Image with index {} has label {}'.format(idx, label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SZax14Sg61s"
      },
      "source": [
        "### Question 4 (5 points)\n",
        "\n",
        "What is the number of classes in the MNIST dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HIaY37ozhB7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f44f00-3745-4c4e-e64b-d335f1b730c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 10 classes\n"
          ]
        }
      ],
      "source": [
        "def get_num_classes(dataset):\n",
        "  # TODO: Implement a function that takes a dataset object\n",
        "  # and returns the number of classes for that dataset.\n",
        "\n",
        "  num_classes = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  num_classes = len(torch.unique(dataset.targets if hasattr(dataset, 'targets') else dataset.dataset.targets[dataset.indices]))\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return num_classes\n",
        "\n",
        "num_classes = get_num_classes(train_data)\n",
        "print(\"dataset has {} classes\".format(num_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zb0ZDyp2_QX"
      },
      "source": [
        "We can have a look at these images with the help of library `matplotlib`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ZZTKpDUW2kOo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "d587adfe-9a05-4662-89b4-1e580a39ece8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_idx = 50  # the index of the image we want to see\n",
        "image = train_data[50][0]\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.imshow(image.view(28, 28).cpu().numpy(), cmap='bone')  # plot the image\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJmfZ8Ll4jCi"
      },
      "source": [
        "`DataLoader` is also an important tool during our training. It can iterate over the dataset and yield a batch of images and labels to the model. The batchsize is a hyperparameter and we need to tune it during training. Here we set batchsize as 64, and define the dataloader for training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "c0wuXZZj6FX1"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = data.DataLoader(train_data,\n",
        "                                 shuffle=True,  # shuffle the dataset in every epoch\n",
        "                                 batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "val_loader = data.DataLoader(val_data,\n",
        "                                batch_size=BATCH_SIZE)\n",
        "test_loader = data.DataLoader(test_data,\n",
        "                                batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6Clt1fXZKtJ"
      },
      "source": [
        "Now we need to initialize a MLP with appropriate `input_dim`, `hidden_dim` and `output_dim`. The image size is 28*28, and we will first flatten the image into a 784 element vector. So `input_dim` is 784 in MLP. `hidden_dim` is a tunable hyperparameter, and here we set it as 200. `output_dim` is the number of the classes. Please write the correct numbers and initialize a MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "i4mVaLnZZlAt"
      },
      "outputs": [],
      "source": [
        "input_dim, hidden_dim, output_dim = 784, 200, 10\n",
        "\n",
        "model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P68ox-lD6yvE"
      },
      "source": [
        "## Training and testing\n",
        "\n",
        "Here is a total pipeline of training a model:\n",
        "\n",
        "- pass a batch of data through the model and obtain the prediction\n",
        "- compare the prediction with the label and calculate the loss of this batch\n",
        "- calculate the gradient of each of the parameters with respect to the loss\n",
        "- update the parameters with optimizer\n",
        "\n",
        "To end-to-end train a model, we need to define a [optimizer](https://ruder.io/optimizing-gradient-descent/) and [loss function](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23). Optimizer is a kind of algorithm used to updated the parameter effectively. Here we use Adam as our optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "j_AgDChN8pju"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOpgNJT59-GU"
      },
      "source": [
        "Then, we define loss function. How to choose a loss function? It depends on our current machine learning task:\n",
        "\n",
        "- Mean Square Error: measured as the average of squared difference between predictions and actual observations. It is for regression task.\n",
        "- Cross Entropy loss: computes the softmax activation function on the supplied predictions as well as the actual loss via negative log likelihood. It is for classification task.\n",
        "- Mean Absolute Error: measured as the average of sum of absolute differences between predictions and actual observations. It is for regression task.\n",
        "\n",
        "Our task is try to categorize the digit of the image, which is a kind of classification task. So here we define the Cross Entropy loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "_ylJMn0OBXVa"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0y35_83BmjO"
      },
      "source": [
        "Now we can start to train our model! What we will do is:\n",
        "\n",
        "- we put the model into `train` mode\n",
        "- iterate over our dataloader, returning batches of (image, label)\n",
        "- clear the gradients calculated from the last batch\n",
        "- calculate the loss between our predictions and the actual labels\n",
        "- calculate the gradients of each parameter\n",
        "- update the parameters by taking an optimizer step\n",
        "- update our metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "bsKhLiOVCdow"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for (x, y) in tqdm(dataloader, desc=\"Training\"):  # iterate over our dataloader\n",
        "\n",
        "        optimizer.zero_grad()  # clear the gradients\n",
        "\n",
        "        y_pred = model(x)\n",
        "\n",
        "        loss = loss_fn(y_pred, y)  # calculate the loss\n",
        "\n",
        "        loss.backward()  # calculate the gradients of each parameter\n",
        "\n",
        "        optimizer.step()  # update the parameters by taking an optimizer step\n",
        "\n",
        "        # reweight the loss by the number of samples in the batch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqkSkLWKC7ku"
      },
      "source": [
        "### Question 5 (5 points)\n",
        "\n",
        "After training the model, we need to evaluate it. Here we first need to define a function to calculate the accuracy. It will compare the model prediction against the real class label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "CopI23pyDgFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2295e1-d029-4fdd-afe3-2940086b17e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.800000011920929\n"
          ]
        }
      ],
      "source": [
        "def calculate_accuracy(pred, label):\n",
        "  # TODO: Implement the accuracy function. This function takes the\n",
        "  # pred tensor and the label tensor. Take the index of the highest value for\n",
        "  # your prediction and compares it against the actual class label.\n",
        "\n",
        "  acc = 0.0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  #print(pred)\n",
        "  #print(label)\n",
        "  predicted_classes = pred.argmax(dim=1)\n",
        "  #print(predicted_classes)\n",
        "  acc = (predicted_classes == label).float().mean().item()\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return acc\n",
        "\n",
        "pred = torch.rand(5, 3)  # 5 examples, 3 classes\n",
        "label = torch.tensor([0, 1, 2, 1, 0])  # the true label of each example\n",
        "print(\"accuracy:\", calculate_accuracy(pred, label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ope6QD8w8w3S"
      },
      "source": [
        "### Question 6 (10 points)\n",
        "\n",
        "Then we define the evaluation function, which is similar as training one:\n",
        "\n",
        "- we put the model into `eval` mode\n",
        "- we wrap the iterations inside a `with torch.no_grad()`\n",
        "- iterate over our dataloader, returning batches of (image, label)\n",
        "- calculate the loss between our predictions and the actual labels\n",
        "- calculate the accuracy\n",
        "- we do not calculate or zero gradients\n",
        "- we do not take an optimizer step\n",
        "- update our metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "E_qC8okeEG8R"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, loss_fn):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # TODO: Implement a evaluation function that takes the model,\n",
        "    # dataloader, loss function and returns the average loss and accuracy.\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~8 lines of code)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            predictions = model(images)\n",
        "            loss = loss_fn(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += calculate_accuracy(predictions, labels)\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx4JS2VUWzOl"
      },
      "source": [
        "The main difference between training process and evaluating process is: We do not update the weights as they are already trained. We simply iterate over batches obtained by dataloader and get the predictions to find loss and accuracy\n",
        "\n",
        "\n",
        "\n",
        "Let's start to train our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "t7ghay5uYKOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7730e9-4adc-4972-a42a-047088314aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [00:28<00:00, 27.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 28.438212156295776s\n",
            "\tTrain Loss: 0.114\n",
            "Val Loss: 0.126 | Val Acc: 96.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [00:26<00:00, 29.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Epoch Time: 26.966303825378418s\n",
            "\tTrain Loss: 0.106\n",
            "Val Loss: 0.113 | Val Acc: 96.54%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [00:27<00:00, 28.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Epoch Time: 27.145050287246704s\n",
            "\tTrain Loss: 0.102\n",
            "Val Loss: 0.108 | Val Acc: 96.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [00:27<00:00, 28.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Epoch Time: 27.430373191833496s\n",
            "\tTrain Loss: 0.100\n",
            "Val Loss: 0.107 | Val Acc: 96.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [00:27<00:00, 28.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Epoch Time: 27.17013931274414s\n",
            "\tTrain Loss: 0.091\n",
            "Val Loss: 0.100 | Val Acc: 97.08%\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "\n",
        "EPOCHS = 5  # number of training epoch\n",
        "CHECK_VAL_EVERY_N_EPOCH = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    start_time = time()  # record the start time\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "\n",
        "    end_time = time()\n",
        "\n",
        "    epoch_time = end_time - start_time\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_time}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "\n",
        "    if epoch % CHECK_VAL_EVERY_N_EPOCH == 0:\n",
        "      val_loss, val_acc = evaluate(model, val_loader, loss_fn)\n",
        "      print(f'Val Loss: {val_loss:.3f} | Val Acc: {val_acc*100:.2f}%')\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'model.pt')  # saving model's parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvWnjDoRdrEK"
      },
      "source": [
        "After 5 epochs' training, we can load the parameter and evaluate the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "UizGWUDPd4b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a125fca2-1867-44a5-98c9-fc4499bab5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-3c2ace7f4f37>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('model.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.056 | Test Acc: 98.14%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, loss_fn)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5mvZ5q4Szgd"
      },
      "source": [
        "## Submission\n",
        "\n",
        "Make sure to run all the cells and save a copy of this colab in your driver. If you complete this notebook, download the colab and upload your work to canvas to submit it.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "colab1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}