{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Graphormer\n",
        "\n",
        "In this notebook, we'll train a graphormer (graph transformer) to perform a graph-level task on a molecule dataset. Specifically, we will be able to predict various attributes of molecules, including the toxicity of each molecule on 12 different targets, as well as the water solubility of each molecule.\n",
        "\n",
        "We will be using the OGB Molecule dataset for training and evaluation."
      ],
      "metadata": {
        "id": "KuqVw2Nri02a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Package installation\n",
        "\n",
        "We first install the required packages. Note that installation might take some time."
      ],
      "metadata": {
        "id": "omtz_S8ji3lW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-pjyvYNerj_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b255e64-4d1d-45b0-e8ea-48eed6db20da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_scatter-2.1.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_sparse-0.6.18%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_cluster-1.6.3%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt25cu124\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-cg9ijll9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-cg9ijll9\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit f506499f3bc0c227c882704e0a164b07213cd6a3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.11.12)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (4.67.1)\n",
            "Collecting xxhash (from torch-geometric==2.7.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2025.1.31)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1179629 sha256=2300fa90008e37d9e6726538f1397101e33e7650daee5950613bc7c9b1a67554\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1y64qwsm/wheels/93/bb/85/bfec4ee59b2563f74ec87cc2c91c6a4d3e40d3dcdec8ee5afe\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: xxhash, torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0 xxhash-3.5.0\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.3.0)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.1.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.1.31)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m830.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, littleutils, outdated, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ogb\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed littleutils-0.2.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install ogb  # for datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cython"
      ],
      "metadata": {
        "id": "slX8w1tmi4zb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a59b4a7-5de4-4638-aec2-df65f0bc5a01"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (3.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cython"
      ],
      "metadata": {
        "id": "jwNznjAFi7Sb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipeline"
      ],
      "metadata": {
        "id": "36dxdHrTi9Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we implement a few useful functions using cython to accelerate our pipeline. It is not required for you to read or understand the code below.\n"
      ],
      "metadata": {
        "id": "xavHHsdyi-tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cython\n",
        "\n",
        "import cython\n",
        "from cython.parallel cimport prange, parallel\n",
        "cimport numpy\n",
        "import numpy\n",
        "\n",
        "## Floyd Warshall computes the shortest path in a weighted graph\n",
        "def floyd_warshall(adjacency_matrix):\n",
        "\n",
        "    (nrows, ncols) = adjacency_matrix.shape\n",
        "    assert nrows == ncols\n",
        "    cdef unsigned int n = nrows\n",
        "\n",
        "    adj_mat_copy = adjacency_matrix.astype(long, order='C', casting='safe', copy=True)\n",
        "    assert adj_mat_copy.flags['C_CONTIGUOUS']\n",
        "    cdef numpy.ndarray[long, ndim=2, mode='c'] M = adj_mat_copy\n",
        "    cdef numpy.ndarray[long, ndim=2, mode='c'] path = numpy.zeros([n, n], dtype=numpy.int64)\n",
        "\n",
        "    cdef unsigned int i, j, k\n",
        "    cdef long M_ij, M_ik, cost_ikkj\n",
        "    cdef long* M_ptr = &M[0,0]\n",
        "    cdef long* M_i_ptr\n",
        "    cdef long* M_k_ptr\n",
        "\n",
        "    # set unreachable nodes distance to 510\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                M[i][j] = 0\n",
        "            elif M[i][j] == 0:\n",
        "                M[i][j] = 510\n",
        "\n",
        "    # floyed algo\n",
        "    for k in range(n):\n",
        "        M_k_ptr = M_ptr + n*k\n",
        "        for i in range(n):\n",
        "            M_i_ptr = M_ptr + n*i\n",
        "            M_ik = M_i_ptr[k]\n",
        "            for j in range(n):\n",
        "                cost_ikkj = M_ik + M_k_ptr[j]\n",
        "                M_ij = M_i_ptr[j]\n",
        "                if M_ij > cost_ikkj:\n",
        "                    M_i_ptr[j] = cost_ikkj\n",
        "                    path[i][j] = k\n",
        "\n",
        "    # set unreachable path to 510\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if M[i][j] >= 510:\n",
        "                path[i][j] = 510\n",
        "                M[i][j] = 510\n",
        "\n",
        "    return M, path\n",
        "\n",
        "\n",
        "def get_all_edges(path, i, j):\n",
        "    cdef unsigned int k = path[i][j]\n",
        "    if k == 0:\n",
        "        return []\n",
        "    else:\n",
        "        return get_all_edges(path, i, k) + [k] + get_all_edges(path, k, j)\n",
        "\n",
        "\n",
        "## Generates a 4D tensor of shape (num_nodes, num_nodes, max_path_length, edge_feature_size)\n",
        "## [i,j,k,:] stores the node features of the edge from path[k] and path[k+1] where path is the shortest path between i and j.\n",
        "def gen_edge_input(max_dist, path, edge_feat):\n",
        "\n",
        "    (nrows, ncols) = path.shape\n",
        "    assert nrows == ncols\n",
        "    cdef unsigned int n = nrows\n",
        "    cdef unsigned int max_dist_copy = max_dist\n",
        "\n",
        "    path_copy = path.astype(long, order='C', casting='safe', copy=True)\n",
        "    edge_feat_copy = edge_feat.astype(long, order='C', casting='safe', copy=True)\n",
        "    assert path_copy.flags['C_CONTIGUOUS']\n",
        "    assert edge_feat_copy.flags['C_CONTIGUOUS']\n",
        "\n",
        "    cdef numpy.ndarray[long, ndim=4, mode='c'] edge_fea_all = -1 * numpy.ones([n, n, max_dist_copy, edge_feat.shape[-1]], dtype=numpy.int64)\n",
        "    cdef unsigned int i, j, k, num_path, cur\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                continue\n",
        "            if path_copy[i][j] == 510:\n",
        "                continue\n",
        "            path = [i] + get_all_edges(path_copy, i, j) + [j]\n",
        "            num_path = len(path) - 1\n",
        "            for k in range(num_path):\n",
        "                edge_fea_all[i, j, k, :] = edge_feat_copy[path[k], path[k+1], :]\n",
        "\n",
        "    return edge_fea_all"
      ],
      "metadata": {
        "id": "z1NOI6RWi8bU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ba3452-cac7-4620-d0c7-9eafb8c37313"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of stderr:\n",
            "In file included from /usr/local/lib/python3.11/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1929,\n",
            "                 from /usr/local/lib/python3.11/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12,\n",
            "                 from /usr/local/lib/python3.11/dist-packages/numpy/core/include/numpy/arrayobject.h:5,\n",
            "                 from /root/.cache/ipython/cython/_cython_magic_21c102fac3eb95e5053ceda14e0096f57188c0bc.c:1250:\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
            "   17 | #warning \"Using deprecated NumPy API, disable it with \" \\\n",
            "      |  ^~~~~~~"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 points)\n",
        "\n",
        "The code below defines some functions to preprocess the graphs in our dataset for use in the graphormer.\n",
        "\n",
        "In the relevant section of the code below, construct a Boolean adjacency matrix `adj` using the `edge_index` attribute of the input graph."
      ],
      "metadata": {
        "id": "mMgb2kTAjJoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "## This function is used to ensure that each feature is non-overlapping.\n",
        "def convert_to_single_emb(x, offset=512):\n",
        "    feature_num = x.size(1) if len(x.size()) > 1 else 1\n",
        "    feature_offset = 1 + torch.arange(0, feature_num * offset, offset, dtype=torch.long)\n",
        "    x = x + feature_offset\n",
        "    return x\n",
        "\n",
        "## Preprocesses each graph for use in model\n",
        "def preprocess_item(item):\n",
        "    ## item is a graph in our dataset.\n",
        "    edge_attr, edge_index, x = item.edge_attr, item.edge_index, item.x\n",
        "\n",
        "    N = x.size(0)\n",
        "    x = convert_to_single_emb(x)\n",
        "\n",
        "    ## Question 1: Construct an adjacency matrix of size (N, N) named 'adj' and dtype=torch.bool using edge_index\n",
        "    ############# Your code here ############\n",
        "    ## (~2 lines of code)\n",
        "\n",
        "    adj = torch.zeros((N, N), dtype=torch.bool)\n",
        "    adj[edge_index[0], edge_index[1]] = True\n",
        "\n",
        "    #########################################\n",
        "\n",
        "\n",
        "    ## edge_attr is of shape (num_edges, edge_feature_dim)\n",
        "    ## attn_edge_type is of shape (N, N, edge_feature_dim)\n",
        "    if len(edge_attr.size()) == 1:\n",
        "        edge_attr = edge_attr[:, None]\n",
        "    attn_edge_type = torch.zeros([N, N, edge_attr.size(-1)], dtype=torch.long)\n",
        "    attn_edge_type[edge_index[0, :], edge_index[1, :]] = convert_to_single_emb(edge_attr) + 1\n",
        "\n",
        "    shortest_path_result, path = floyd_warshall(adj.numpy())\n",
        "    max_dist = np.amax(shortest_path_result)\n",
        "    edge_input = gen_edge_input(max_dist, path, attn_edge_type.numpy())\n",
        "    rel_pos = torch.from_numpy((shortest_path_result)).long()\n",
        "    attn_bias = torch.zeros(\n",
        "        [N + 1, N + 1], dtype=torch.float)  # + 1 to account for graph-level token\n",
        "\n",
        "    # Add the relevant tensors as attributes of the graph\n",
        "    item.x = x\n",
        "    item.adj = adj\n",
        "    item.attn_bias = attn_bias\n",
        "    item.attn_edge_type = attn_edge_type\n",
        "    item.rel_pos = rel_pos\n",
        "    item.in_degree = adj.long().sum(dim=1).view(-1)\n",
        "    item.out_degree = adj.long().sum(dim=0).view(-1)\n",
        "    item.edge_input = torch.from_numpy(edge_input).long()\n",
        "\n",
        "    return item"
      ],
      "metadata": {
        "id": "R8xApNzcjBpc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test the function `preprocess_item` using the code below."
      ],
      "metadata": {
        "id": "Lx5xM2BxkWTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "edge_index = torch.tensor([[0, 1, 1, 2],\n",
        "                       [1, 0, 2, 1]], dtype=torch.long)\n",
        "edge_attr = torch.rand(4, 3).long()\n",
        "x = torch.tensor([[-1], [0], [1]], dtype=torch.long)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "data = preprocess_item(data)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "uq4WyOzMjLrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e74d278-5585-42b8-f2ea-2e91f84e7873"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[3, 1], edge_index=[2, 4], edge_attr=[4, 3], adj=[3, 3], attn_bias=[4, 4], attn_edge_type=[3, 3, 3], rel_pos=[3, 3], in_degree=[3], out_degree=[3], edge_input=[3, 3, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we define some useful methods and the Batch class that will be used in the upcoming 'collate' function that pools the attributes of multiple graphs into a Batch that can be trained on."
      ],
      "metadata": {
        "id": "z5T6xOTxkdJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_1d_unsqueeze(x, padlen):\n",
        "    x = x + 1  # pad id = 0\n",
        "    xlen = x.size(0)\n",
        "    if xlen < padlen:\n",
        "        new_x = x.new_zeros([padlen], dtype=x.dtype)\n",
        "        new_x[:xlen] = x\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def pad_2d_unsqueeze(x, padlen):\n",
        "    x = x + 1  # pad id = 0\n",
        "    xlen, xdim = x.size()\n",
        "    if xlen < padlen:\n",
        "        new_x = x.new_zeros([padlen, xdim], dtype=x.dtype)\n",
        "        new_x[:xlen, :] = x\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def pad_attn_bias_unsqueeze(x, padlen):\n",
        "    xlen = x.size(0)\n",
        "    if xlen < padlen:\n",
        "        new_x = x.new_zeros(\n",
        "            [padlen, padlen], dtype=x.dtype).fill_(float('-inf'))\n",
        "        new_x[:xlen, :xlen] = x\n",
        "        new_x[xlen:, :xlen] = 0\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def pad_edge_type_unsqueeze(x, padlen):\n",
        "    xlen = x.size(0)\n",
        "    if xlen < padlen:\n",
        "        new_x = x.new_zeros([padlen, padlen, x.size(-1)], dtype=x.dtype)\n",
        "        new_x[:xlen, :xlen, :] = x\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def pad_rel_pos_unsqueeze(x, padlen):\n",
        "    x = x + 1\n",
        "    xlen = x.size(0)\n",
        "    if xlen < padlen:\n",
        "        new_x = x.new_zeros([padlen, padlen], dtype=x.dtype)\n",
        "        new_x[:xlen, :xlen] = x\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def pad_3d_unsqueeze(x, padlen1, padlen2, padlen3):\n",
        "    x = x + 1\n",
        "    xlen1, xlen2, xlen3, xlen4 = x.size()\n",
        "    if xlen1 < padlen1 or xlen2 < padlen2 or xlen3 < padlen3:\n",
        "        new_x = x.new_zeros([padlen1, padlen2, padlen3, xlen4], dtype=x.dtype)\n",
        "        new_x[:xlen1, :xlen2, :xlen3, :] = x\n",
        "        x = new_x\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "class Batch():\n",
        "    def __init__(self, attn_bias, attn_edge_type, rel_pos, in_degree, out_degree, x, edge_input, y):\n",
        "        super(Batch, self).__init__()\n",
        "        self.in_degree, self.out_degree = in_degree, out_degree\n",
        "        self.x, self.y = x, y\n",
        "        self.attn_bias, self.attn_edge_type, self.rel_pos = attn_bias, attn_edge_type, rel_pos\n",
        "        self.edge_input = edge_input\n",
        "\n",
        "    def to(self, device):\n",
        "        self.in_degree, self.out_degree = self.in_degree.to(\n",
        "            device), self.out_degree.to(device)\n",
        "        self.x, self.y = self.x.to(device), self.y.to(device)\n",
        "        self.attn_bias, self.attn_edge_type, self.rel_pos = self.attn_bias.to(\n",
        "            device), self.attn_edge_type.to(device), self.rel_pos.to(device)\n",
        "        self.edge_input = self.edge_input.to(device)\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.in_degree.size(0)\n",
        "\n",
        "    def __str__(self):\n",
        "      return \"Batch(x={}, y={}, attn_bias={}, attn_edge_type={}, rel_pos={}, edge_input={})\".format(\n",
        "          self.x.size(),\n",
        "          self.y.size(),\n",
        "          self.attn_bias.size(),\n",
        "          self.attn_edge_type.size(),\n",
        "          self.rel_pos.size(),\n",
        "          self.edge_input.size()\n",
        "      )"
      ],
      "metadata": {
        "id": "ePh2gKY2kbaT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to implement the `collator` function, which will be used to generate batches for training our graphormer. `collator` takes as input a set of graphs, and returns a `Batch` object that contains the relevant attributes of these graphs in a format that can be used for training."
      ],
      "metadata": {
        "id": "4QgtEZUnkjlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (5 points) and 3 (10 points)\n",
        "\n",
        "Question 2: Filters `items` to only include graphs that are not `None` and that have less nodes than `max_node`. Note that this should change `items` itself so it can be used later in the code.\n",
        "\n",
        "Question 3: Using a padding function defined above, pad `x` and `edge_inputs` upto the correct sizes, stipulated by `max_node` and `multi_hop_max_dist`. Ensure that you understand what `x` and `edge_inputs` are supposed to represent, their dimensions, and how they should be modified to be used in creating the `Batch` object to get this question correct.\n"
      ],
      "metadata": {
        "id": "hqrFFkeUkk9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(items, max_node=512, multi_hop_max_dist=20, rel_pos_max=20):\n",
        "\n",
        "    ## Question 2: Filter out graphs that are too large\n",
        "    ############# Your code here ############\n",
        "    ## (~1-2 lines of code)\n",
        "\n",
        "    items = [item for item in items if item is not None and item.x.size(0) < max_node]\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    if len(items) == 0:\n",
        "        return None\n",
        "\n",
        "    ## Collect the relevant attributes of each graph\n",
        "    items = [(item.attn_bias, item.attn_edge_type, item.rel_pos, item.in_degree,\n",
        "              item.out_degree, item.x, item.edge_input[:, :, :multi_hop_max_dist, :], item.y) for item in items]\n",
        "    attn_biases, attn_edge_types, rel_poses, in_degrees, out_degrees, xs, edge_inputs, ys = zip(\n",
        "        *items)\n",
        "\n",
        "    ## Set the attention biases of nodes far away from the current node to -inf\n",
        "    ## This masks out distant nodes' attention contributions to the node at hand.\n",
        "    for idx, _ in enumerate(attn_biases):\n",
        "        attn_biases[idx][1:, 1:][rel_poses[idx] >= rel_pos_max] = float('-inf')\n",
        "\n",
        "    ## Find the largest number of nodes and largest distance to pad all other graphs' attributes to this length\n",
        "    max_node_num = max(i.size(0) for i in xs)\n",
        "    max_dist = max(i.size(-2) for i in edge_inputs)\n",
        "\n",
        "    y = torch.cat(ys)\n",
        "    ## Question 3: use the pad function to pad x and edge_inputs as used below\n",
        "    ############# Your code here ############\n",
        "    ## (~2-4 lines of code)\n",
        "\n",
        "    # Pad node features x to [max_node_num, d] for each graph.\n",
        "    x = torch.cat([pad_2d_unsqueeze(i, max_node_num) for i in xs])\n",
        "\n",
        "    # Pad edge_input to [max_node_num, max_node_num, multi_hop_max_dist, f]\n",
        "    edge_input = torch.cat([pad_3d_unsqueeze(i, max_node_num, max_node_num, multi_hop_max_dist) for i in edge_inputs])\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    attn_bias = torch.cat([pad_attn_bias_unsqueeze(\n",
        "        i, max_node_num + 1) for i in attn_biases])\n",
        "    attn_edge_type = torch.cat(\n",
        "        [pad_edge_type_unsqueeze(i, max_node_num) for i in attn_edge_types])\n",
        "    rel_pos = torch.cat([pad_rel_pos_unsqueeze(i, max_node_num)\n",
        "                        for i in rel_poses])\n",
        "    in_degree = torch.cat([pad_1d_unsqueeze(i, max_node_num)\n",
        "                          for i in in_degrees])\n",
        "    out_degree = torch.cat([pad_1d_unsqueeze(i, max_node_num)\n",
        "                           for i in out_degrees])\n",
        "\n",
        "    return Batch(\n",
        "        attn_bias=attn_bias,\n",
        "        attn_edge_type=attn_edge_type,\n",
        "        rel_pos=rel_pos,\n",
        "        in_degree=in_degree,\n",
        "        out_degree=out_degree,\n",
        "        x=x,\n",
        "        edge_input=edge_input,\n",
        "        y=y,\n",
        "    )"
      ],
      "metadata": {
        "id": "SW8WsI7Uki9I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the following code to check your `collator` function:"
      ],
      "metadata": {
        "id": "4lTr3lOykqZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "edge_index1 = torch.tensor([[0, 1, 1, 2],\n",
        "                       [1, 0, 2, 1]], dtype=torch.long)\n",
        "edge_attr1 = torch.rand(4, 3).long()\n",
        "x1 = torch.tensor([[-1], [0], [1]], dtype=torch.long)\n",
        "y1 = torch.tensor([1], dtype=torch.float)\n",
        "edge_index2 = torch.tensor([[0, 1, 1, 2, 3, 1],\n",
        "                       [1, 0, 2, 1, 1, 3]], dtype=torch.long)\n",
        "edge_attr2 = torch.rand(6, 3).long()\n",
        "x2 = torch.tensor([[-1], [0], [1], [2]], dtype=torch.long)\n",
        "y2 = torch.tensor([0], dtype=torch.float)\n",
        "data1 = Data(x=x1, edge_index=edge_index1, edge_attr=edge_attr1, y=y1)\n",
        "data1 = preprocess_item(data1)\n",
        "data2 = Data(x=x2, edge_index=edge_index2, edge_attr=edge_attr2, y=y2)\n",
        "data2 = preprocess_item(data2)\n",
        "\n",
        "batch = collator([data1, data2])\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "fnpK1Lz9kndo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c817f069-7c02-4ca8-c5c8-f79a73d7dfc9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch(x=torch.Size([2, 4, 1]), y=torch.Size([2]), attn_bias=torch.Size([2, 5, 5]), attn_edge_type=torch.Size([2, 4, 4, 3]), rel_pos=torch.Size([2, 4, 4]), edge_input=torch.Size([2, 4, 4, 20, 3]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset + Dataloader\n",
        "\n",
        "In this section, we make a custom Dataset class for our graphs, and then define a dataloader that will be used in the training code."
      ],
      "metadata": {
        "id": "1i6QLyxUk2hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GraphormerDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "\n",
        "        self.num = len(dataset)\n",
        "        self.dataset = dataset\n",
        "        self.indices = torch.arange(self.num)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        ## Directly return the sampled graph\n",
        "        sampled_graph = self.dataset[self.indices[item]]\n",
        "        return preprocess_item(sampled_graph)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num\n",
        "\n",
        "    def shuffle(self):\n",
        "        rand = torch.randperm(self.num)\n",
        "        self.indices = self.indices[rand]"
      ],
      "metadata": {
        "id": "9XlGm0iRkrmw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "class GraphormerDataLoader(DataLoader):\n",
        "    def __init__(self, dataset, **kwargs):\n",
        "        self.dataset = GraphormerDataset(dataset)\n",
        "        self.collator = partial(collator, max_node=128, multi_hop_max_dist=5, rel_pos_max=1024)\n",
        "        kwargs[\"collate_fn\"] = self.__collate_fn__\n",
        "        super().__init__(dataset=self.dataset, **kwargs)\n",
        "\n",
        "    ## This function allows graphs to be grouped into batches during training, using the Batch class defined above\n",
        "    def __collate_fn__(self, batch):\n",
        "        batch_graphs = batch\n",
        "        batch_graphs = self.collator(batch_graphs)  # make the sampled graphs a batch\n",
        "        return batch_graphs"
      ],
      "metadata": {
        "id": "J0L0ckOOk30-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the Graphormer"
      ],
      "metadata": {
        "id": "Ed1cIMBlk5WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we define a few simple modules to define MLP and multi-head attention layers, combining them into a single transformer layer (`EncoderLayer`)."
      ],
      "metadata": {
        "id": "omFRsV_Dk6ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, hidden_size, ffn_size):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(hidden_size, ffn_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.layer2 = nn.Linear(ffn_size, hidden_size)\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "## Implements multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, attention_dropout_rate, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.att_size = att_size = hidden_size // num_heads\n",
        "        self.scale = att_size ** -0.5\n",
        "\n",
        "        self.linear_q = nn.Linear(hidden_size, num_heads * att_size)\n",
        "        self.linear_k = nn.Linear(hidden_size, num_heads * att_size)\n",
        "        self.linear_v = nn.Linear(hidden_size, num_heads * att_size)\n",
        "        self.att_dropout = nn.Dropout(attention_dropout_rate)\n",
        "\n",
        "        self.input_norm = nn.LayerNorm(hidden_size)\n",
        "        self.output_layer = nn.Linear(num_heads * att_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, attn_bias=None):\n",
        "        orig_q_size = x.size()\n",
        "\n",
        "        x = self.input_norm(x)\n",
        "\n",
        "        d_k = self.att_size\n",
        "        d_v = self.att_size\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n",
        "        q = self.linear_q(x).view(batch_size, -1, self.num_heads, d_k)\n",
        "        k = self.linear_k(x).view(batch_size, -1, self.num_heads, d_k)\n",
        "        v = self.linear_v(x).view(batch_size, -1, self.num_heads, d_v)\n",
        "\n",
        "        q = q.transpose(1, 2)                  # [b, h, q_len, d_k]\n",
        "        v = v.transpose(1, 2)                  # [b, h, v_len, d_v]\n",
        "        k = k.transpose(1, 2).transpose(2, 3)  # [b, h, d_k, k_len]\n",
        "\n",
        "        # Scaled Dot-Product Attention.\n",
        "        # Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\n",
        "        q = q * self.scale\n",
        "        x = torch.matmul(q, k)  # [b, h, q_len, k_len]\n",
        "        if attn_bias is not None:\n",
        "            x = x + attn_bias\n",
        "\n",
        "        x = torch.softmax(x, dim=3)\n",
        "        x = self.att_dropout(x)\n",
        "        x = x.matmul(v)  # [b, h, q_len, attn]\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous()  # [b, q_len, h, attn]\n",
        "        x = x.view(batch_size, -1, self.num_heads * d_v)\n",
        "\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        assert x.size() == orig_q_size\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, ffn_size, dropout_rate, attention_dropout_rate, num_heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(hidden_size, attention_dropout_rate, num_heads)\n",
        "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.ffn = FeedForwardNetwork(hidden_size, ffn_size)\n",
        "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, attn_bias=None):\n",
        "        y = self.self_attn(x, attn_bias)\n",
        "        y = self.self_attention_dropout(y)\n",
        "        x1 = x + y\n",
        "        y = self.ffn(x1)\n",
        "        y = self.ffn_dropout(y)\n",
        "        return x1 + y ## Residual stream"
      ],
      "metadata": {
        "id": "2aZR7WAfk5Al"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 (10 points)\n",
        "\n",
        "Implement the part of the forward method of the Graphormer that actually does the forward pass through the encoder layers. Remember that the `forward` method of each Encoder module takes as an argument an `attn_bias`. Also, remember to apply a final LayerNorm (`self.final_ln`) on the output of the Graphormer before returning the result."
      ],
      "metadata": {
        "id": "hes85kAFk__I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "## Initializes params randomly\n",
        "def init_bert_params(module, n_layers):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(mean=0.0, std=0.02 / math.sqrt(n_layers))\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    if isinstance(module, nn.Embedding):\n",
        "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "\n",
        "class Graphormer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_layers,\n",
        "        num_heads,\n",
        "        hidden_dim,\n",
        "        dropout_rate,\n",
        "        input_dropout_rate,\n",
        "        ffn_dim,\n",
        "        edge_type,\n",
        "        multi_hop_max_dist,\n",
        "        attention_dropout_rate,\n",
        "        node_feature_num,\n",
        "        edge_feature_num\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        ## Generates an embedding for nodes and edges\n",
        "        self.atom_encoder = nn.Embedding(512 * node_feature_num + 1, hidden_dim, padding_idx=0) ## Assuming max_number of nodes is 512\n",
        "        self.edge_encoder = nn.Embedding(512 * edge_feature_num + 1, num_heads, padding_idx=0)\n",
        "\n",
        "        self.edge_type = edge_type\n",
        "        if self.edge_type == 'multi_hop':\n",
        "            self.edge_dis_encoder = nn.Embedding(128 * num_heads * num_heads, 1)\n",
        "        self.rel_pos_encoder = nn.Embedding(512, num_heads, padding_idx=0)\n",
        "        self.in_degree_encoder = nn.Embedding(512, hidden_dim, padding_idx=0)\n",
        "        self.out_degree_encoder = nn.Embedding(512, hidden_dim, padding_idx=0)\n",
        "        self.input_dropout = nn.Dropout(input_dropout_rate)\n",
        "\n",
        "        ## Define the layers of the transformer\n",
        "        encoders = [EncoderLayer(hidden_dim, ffn_dim, dropout_rate, attention_dropout_rate, num_heads)\n",
        "                    for _ in range(n_layers)]\n",
        "        self.layers = nn.ModuleList(encoders)\n",
        "        self.final_ln = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        ## Define the graph-level token\n",
        "        self.graph_token = nn.Embedding(1, hidden_dim)\n",
        "        self.graph_token_virtual_distance = nn.Embedding(1, num_heads)\n",
        "\n",
        "        self.multi_hop_max_dist = multi_hop_max_dist\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.apply(lambda module: init_bert_params(module, n_layers=n_layers))\n",
        "\n",
        "    def forward(self, batched_data):\n",
        "        attn_bias, rel_pos, x = batched_data.attn_bias, batched_data.rel_pos, batched_data.x\n",
        "        in_degree, out_degree = batched_data.in_degree, batched_data.in_degree\n",
        "        edge_input, attn_edge_type = batched_data.edge_input, batched_data.attn_edge_type\n",
        "\n",
        "        ## Construct the graph attention bias in multi-head attention\n",
        "        n_graph, n_node = x.size()[:2]\n",
        "        graph_attn_bias = attn_bias.clone()\n",
        "        graph_attn_bias = graph_attn_bias.unsqueeze(1).repeat(\n",
        "            1, self.num_heads, 1, 1)  # [n_graph, n_head, n_node+1, n_node+1]\n",
        "\n",
        "        ## Construct the relative position encoding and add to graph attention bias\n",
        "        # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n",
        "        rel_pos_bias = self.rel_pos_encoder(rel_pos).permute(0, 3, 1, 2)\n",
        "        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + rel_pos_bias  ## Add spatial encoding\n",
        "        ## Include bias to attend to the graph-level token\n",
        "\n",
        "        t = self.graph_token_virtual_distance.weight.view(1, self.num_heads, 1)\n",
        "        graph_attn_bias[:, :, 1:, 0] = graph_attn_bias[:, :, 1:, 0] + t\n",
        "        graph_attn_bias[:, :, 0, :] = graph_attn_bias[:, :, 0, :] + t\n",
        "\n",
        "        ## Construct edge features to be used in MHA\n",
        "        if self.edge_type == 'multi_hop':\n",
        "            rel_pos_ = rel_pos.clone()\n",
        "            rel_pos_[rel_pos_ == 0] = 1  # set pad to 1\n",
        "            # set 1 to 1, x > 1 to x - 1\n",
        "            rel_pos_ = torch.where(rel_pos_ > 1, rel_pos_ - 1, rel_pos_)\n",
        "            if self.multi_hop_max_dist > 0:\n",
        "                rel_pos_ = rel_pos_.clamp(0, self.multi_hop_max_dist)\n",
        "                edge_input = edge_input[:, :, :, :self.multi_hop_max_dist, :]\n",
        "            # [n_graph, n_node, n_node, max_dist, n_head]\n",
        "            edge_input = self.edge_encoder(edge_input).mean(-2)\n",
        "            max_dist = edge_input.size(-2)\n",
        "            edge_input_flat = edge_input.permute(\n",
        "                3, 0, 1, 2, 4).reshape(max_dist, -1, self.num_heads)\n",
        "            edge_input_flat = torch.bmm(edge_input_flat, self.edge_dis_encoder.weight.reshape(\n",
        "                -1, self.num_heads, self.num_heads)[:max_dist, :, :])\n",
        "            edge_input = edge_input_flat.reshape(\n",
        "                max_dist, n_graph, n_node, n_node, self.num_heads).permute(1, 2, 3, 0, 4)\n",
        "            edge_input = (edge_input.sum(-2) /\n",
        "                          (rel_pos_.float().unsqueeze(-1))).permute(0, 3, 1, 2)\n",
        "        else:\n",
        "            # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n",
        "            edge_input = self.edge_encoder(attn_edge_type).mean(-2).permute(0, 3, 1, 2)\n",
        "\n",
        "        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + edge_input  # Add edge encoder\n",
        "        graph_attn_bias = graph_attn_bias + attn_bias.unsqueeze(1)\n",
        "\n",
        "        # node feauture + graph token\n",
        "        node_feature = self.atom_encoder(x).sum(dim=-2)  # [n_graph, n_node, n_hidden] after summing over node features\n",
        "\n",
        "        node_feature = node_feature + \\\n",
        "            self.in_degree_encoder(in_degree) + \\\n",
        "            self.out_degree_encoder(out_degree)  # degree encoder\n",
        "        graph_token_feature = self.graph_token.weight.unsqueeze(\n",
        "            0).repeat(n_graph, 1, 1)\n",
        "        graph_node_feature = torch.cat(\n",
        "            [graph_token_feature, node_feature], dim=1)\n",
        "\n",
        "        ## Question 4: Finish the forward pass\n",
        "        output = self.input_dropout(graph_node_feature)\n",
        "        ############# Your code here ############\n",
        "        ## (~2-4 lines of code)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, graph_attn_bias)\n",
        "        output = self.final_ln(output)\n",
        "\n",
        "        #########################################\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "2PMokqlLk9RM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The following code can be used to check the forward pass:\n",
        "graphormer = Graphormer(n_layers=3,\n",
        "                       num_heads=5,\n",
        "                       hidden_dim=32,\n",
        "                       dropout_rate=0.1,\n",
        "                       input_dropout_rate=0.1,\n",
        "                       ffn_dim=32,\n",
        "                       edge_type=\"multi_hop\",\n",
        "                       multi_hop_max_dist=5,\n",
        "                       attention_dropout_rate=0.1,\n",
        "                       node_feature_num=2,\n",
        "                       edge_feature_num=3,\n",
        "                    )\n",
        "graphormer_output = graphormer(batch)\n",
        "print(graphormer_output.size())"
      ],
      "metadata": {
        "id": "ELty59GnlBv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2abb684-34df-4920-d935-60b4ffb87f2b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder for graph-level tasks\n",
        "\n",
        "The graphormer generates node embeddings for each node including the graph-level token/node that is used for the graph-level task. However, we now have to construct a prediction/decoding head that translates this node embedding into the task output (e.g. solubility of a molecule). To do this, we construct `NNDecoder`, a prediction/classification head for all the tasks."
      ],
      "metadata": {
        "id": "hPTGeg2BlHtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5 (10 points)\n",
        "\n",
        "Implement both the `__init__` and `forward` methods of this decoder class, where the decoder is simply a linear layer from embedding to task space. Remember that we constructed a graph-level token for a reason - this should be used in the decoding."
      ],
      "metadata": {
        "id": "HMgU2sYOlI7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.nn import MessagePassing\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn.inits import uniform\n",
        "\n",
        "from torch_scatter import scatter_mean\n",
        "\n",
        "\n",
        "class NNDecoder(torch.nn.Module):\n",
        "    def __init__(self, num_tasks, emb_dim = 300, graph_pooling = \"mean\"):\n",
        "        super().__init__()\n",
        "        ############# Your code here ############\n",
        "        ## (~2 lines of code)\n",
        "        self.emb_dim = emb_dim\n",
        "        self.decoder = torch.nn.Linear(self.emb_dim, num_tasks)\n",
        "        self.graph_pooling = graph_pooling\n",
        "        #########################################\n",
        "\n",
        "    def forward(self, node_rep):\n",
        "      ## node_rep is the output of the Graphormer module (node embeddings of nodes in the graph for a batch of graphs)\n",
        "      ############# Your code here ############\n",
        "      ## (~2-3 lines of code)\n",
        "\n",
        "      if self.graph_pooling == \"mean\":\n",
        "            graph_rep = node_rep.mean(dim=1)\n",
        "      elif self.graph_pooling == \"sum\":\n",
        "            graph_rep = node_rep.sum(dim=1)\n",
        "      elif self.graph_pooling == \"max\":\n",
        "            graph_rep, _ = node_rep.max(dim=1)\n",
        "      else:\n",
        "            # Default to mean pooling\n",
        "            graph_rep = node_rep.mean(dim=1)\n",
        "\n",
        "      out = self.decoder(graph_rep)\n",
        "\n",
        "      return out\n",
        "      #########################################"
      ],
      "metadata": {
        "id": "vhr-G7h8lFx3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Code to test the decoder:\n",
        "decoder = NNDecoder(2, 32)\n",
        "decoder_output = decoder(graphormer_output)\n",
        "print(decoder_output.size())"
      ],
      "metadata": {
        "id": "33_UoqsulKUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5811c8b-f1f7-4a43-859d-e45322502a95"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Graphormer\n",
        "\n",
        "It's finally time to write our train and test functions. We do this analogously to previous colabs."
      ],
      "metadata": {
        "id": "r3HcSeMHlNC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6 (15 points)\n",
        "\n",
        "Finish the train function. Remember to zero gradients and step both optimizers for your transformer and for the decoder/prediction head. In calculating loss, make sure to use the correct criterion as defined earlier in the function. Finally, remember to only use datapoints with non-NaN y values in calculating your loss (mask the NaNs out of your calculation)."
      ],
      "metadata": {
        "id": "JS2rUnghlOvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "from time import time\n",
        "import numpy as np\n",
        "import logging\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "### Importing OGB dataset\n",
        "from ogb.graphproppred.dataset_pyg import PygGraphPropPredDataset\n",
        "from ogb.graphproppred import Evaluator\n",
        "\n",
        "## The below train and test functions are able to deal with both classification and regression tasks, based on the string parameter of task_type\n",
        "## `'classfication' in task_type` means a classification task, otherwise a regression task\n",
        "def train(epoch, model_list, device, loader, optimizer_list, task_type):\n",
        "    model, decoder = model_list\n",
        "    optimizer, dec_optimizer = optimizer_list\n",
        "\n",
        "    model.train()\n",
        "    decoder.train()\n",
        "\n",
        "    clf_criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    reg_criterion = torch.nn.MSELoss()\n",
        "\n",
        "    loss_list = []\n",
        "    epoch_iter = tqdm(loader, ncols=130)\n",
        "    for step, batch in enumerate(epoch_iter):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## (~10 lines of code)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        dec_optimizer.zero_grad()\n",
        "        node_rep = model(batch)\n",
        "        pred = decoder(node_rep)\n",
        "\n",
        "        if 'classification' in task_type:\n",
        "            loss = clf_criterion(pred, batch.y.float())\n",
        "        else:\n",
        "            loss = reg_criterion(pred, batch.y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        dec_optimizer.step()\n",
        "\n",
        "        #########################################\n",
        "        loss_list.append(loss.item())\n",
        "        epoch_iter.set_description(f\"epoch: {epoch}, train_loss: {loss:.4f}\")\n",
        "\n",
        "    return np.mean(loss_list)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model_list, device, loader, evaluator):\n",
        "    model, decoder = model_list\n",
        "\n",
        "    model.eval()\n",
        "    decoder.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for step, batch in enumerate(loader):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            node_rep = model(batch)\n",
        "            pred = decoder(node_rep)\n",
        "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
        "            y_pred.append(pred.detach().cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "\n",
        "    return evaluator.eval(input_dict)"
      ],
      "metadata": {
        "id": "ghEsD6iplLpS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're finally ready to train our model on datasets of molecules. First, we train on a dataset containing the solubility of molecules in water (a continuous variable)."
      ],
      "metadata": {
        "id": "mG9p6HyBlZnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plotting function for training loss and validation/test metrics\n",
        "def plot_curves(curves):\n",
        "    epochs = range(1, len(curves[\"train\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, curves[\"train\"], label='Training Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xticks(epochs)\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot validation and test metrics\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, curves[\"valid\"], label='Validation Metric', color='orange')\n",
        "    plt.plot(epochs, curves[\"test\"], label='Test Metric', color='green')\n",
        "    plt.title('Validation and Test Metrics over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metric')\n",
        "    plt.xticks(epochs)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "## Code to train model on a particular task\n",
        "def train_model_on_task(task_name, model, node_feature_num, edge_feature_num, num_epochs=20, emb_dim=32, batch_size=32, lr_model=1e-4, lr_dec=1e-4):\n",
        "    device = \"cpu\"\n",
        "    s = time()\n",
        "\n",
        "    ## Load dataset and evaluator\n",
        "    dataset = PygGraphPropPredDataset(name=task_name)\n",
        "    evaluator = Evaluator(task_name)\n",
        "    split_idx = dataset.get_idx_split()\n",
        "\n",
        "    ## Only retain the first node_feature_num node features and first edge_feature_num edges\n",
        "    ## The # of node features in the dataset is 9 and edge features is 3, so can choose some slice of these features.\n",
        "    print(\"Dataset x shape:\", dataset.data.x.shape, \"Dataset edge attr shape:\", dataset.data.edge_attr.shape)\n",
        "    dataset.data.x = dataset.data.x[:,:node_feature_num]\n",
        "    dataset.data.edge_attr = dataset.data.edge_attr[:,:edge_feature_num]\n",
        "\n",
        "    ## Make the split dataloaders\n",
        "    train_loader = GraphormerDataLoader(dataset[split_idx[\"train\"]], batch_size=batch_size, shuffle=True, num_workers = 1)\n",
        "    valid_loader = GraphormerDataLoader(dataset[split_idx[\"valid\"]], batch_size=batch_size, shuffle=False, num_workers = 1)\n",
        "    test_loader = GraphormerDataLoader(dataset[split_idx[\"test\"]], batch_size=batch_size, shuffle=False, num_workers = 1)\n",
        "\n",
        "    ## Define model (transformer and prediction head/decoder) and optimizers\n",
        "    model = model.to(device)\n",
        "    decoder = NNDecoder(emb_dim = emb_dim, num_tasks = dataset.num_tasks).to(device)\n",
        "    model_list = [model, decoder]\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr_model)\n",
        "    dec_optimizer = optim.Adam(decoder.parameters(), lr=lr_dec)\n",
        "    optimizer_list = [optimizer, dec_optimizer]\n",
        "\n",
        "    train_curve = []\n",
        "    valid_curve = []\n",
        "    test_curve = []\n",
        "    train_val_curve = []\n",
        "\n",
        "    ## Train the model and store loss/metrics\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_perf = train(epoch, model_list, device, train_loader, optimizer_list, dataset.task_type)\n",
        "        valid_perf = test(model_list, device, valid_loader, evaluator)\n",
        "        test_perf = test(model_list, device, test_loader, evaluator)\n",
        "\n",
        "        #Replace NaN values with 0.0\n",
        "        valid_perf = {k: np.nan_to_num(v, nan=0.0) for k, v in valid_perf.items()}\n",
        "        test_perf = {k: np.nan_to_num(v, nan=0.0) for k, v in test_perf.items()}\n",
        "\n",
        "        print({'Train Loss': train_perf, 'Validation Metric': valid_perf, 'Test Metric': test_perf})\n",
        "\n",
        "        train_curve.append(np.nan_to_num(train_perf, nan=0.0))\n",
        "        valid_curve.append(valid_perf[dataset.eval_metric])\n",
        "        test_curve.append(test_perf[dataset.eval_metric])\n",
        "\n",
        "    if 'classification' in dataset.task_type:\n",
        "        ## For classification metrics, the higher the better\n",
        "        best_val_epoch = np.argmax(np.array(valid_curve))\n",
        "    else:\n",
        "        ## For regression metrics, the lower the better\n",
        "        best_val_epoch = np.argmin(np.array(valid_curve))\n",
        "\n",
        "    curves = {\"train\": train_curve, \"valid\": valid_curve, \"test\": test_curve}\n",
        "    print('Best Validation Metric: {}'.format(valid_curve[best_val_epoch]))\n",
        "    print('Test Metric: {}'.format(test_curve[best_val_epoch]))\n",
        "    return valid_curve[best_val_epoch], test_curve[best_val_epoch], curves\n",
        "\n",
        "def solubility():\n",
        "    ## Here, our task is to predict the solubility of molecules in water; this is a regression task.\n",
        "    ## Our metric is the mean-squared error between predicted and observed solubility values.\n",
        "    node_feature_num = 4\n",
        "    edge_feature_num = 2\n",
        "    model = Graphormer(n_layers=3,\n",
        "                       num_heads=5,\n",
        "                       hidden_dim=32,\n",
        "                       dropout_rate=0.1,\n",
        "                       input_dropout_rate=0.1,\n",
        "                       ffn_dim=32,\n",
        "                       edge_type=\"multi_hop\",\n",
        "                       multi_hop_max_dist=5,\n",
        "                       attention_dropout_rate=0.1,\n",
        "                       node_feature_num=node_feature_num,\n",
        "                       edge_feature_num=edge_feature_num,\n",
        "                    )\n",
        "    task_name = \"ogbg-molesol\"\n",
        "    return train_model_on_task(task_name, model, node_feature_num, edge_feature_num, num_epochs = 10)\n",
        "\n",
        "val_metric, test_metric, curves = solubility()\n",
        "plot_curves(curves)\n"
      ],
      "metadata": {
        "id": "THqqLOdYlXwg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "outputId": "2c26293f-c64e-4c79-99b8-2adfcdcc5bd4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ogb/graphproppred/dataset_pyg.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset x shape: torch.Size([14991, 9]) Dataset edge attr shape: torch.Size([30856, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 1, train_loss: 2.0396: 100%|███████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  8.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train Loss': 9.509798239017355, 'Validation Metric': {'rmse': 2.828498601913452}, 'Test Metric': {'rmse': 2.9556283950805664}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 2, train_loss: 2.2316: 100%|███████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  8.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train Loss': 4.641562675607616, 'Validation Metric': {'rmse': 2.2411534786224365}, 'Test Metric': {'rmse': 2.380746364593506}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 3, train_loss: 1.7208: 100%|███████████████████████████████████████████████████████████████| 29/29 [00:04<00:00,  6.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train Loss': 3.815756444273324, 'Validation Metric': {'rmse': 2.131265640258789}, 'Test Metric': {'rmse': 2.2729787826538086}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 4, train_loss: 3.0935: 100%|███████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  7.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train Loss': 3.724529348570725, 'Validation Metric': {'rmse': 2.104750394821167}, 'Test Metric': {'rmse': 2.246211051940918}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 5, train_loss: 3.3995: 100%|███████████████████████████████████████████████████████████████| 29/29 [00:04<00:00,  6.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train Loss': 3.6582527982777564, 'Validation Metric': {'rmse': 2.0755255222320557}, 'Test Metric': {'rmse': 2.2166528701782227}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 6, train_loss: 3.5572: 100%|███████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  8.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train Loss': 3.582796964152106, 'Validation Metric': {'rmse': 2.05759334564209}, 'Test Metric': {'rmse': 2.1976001262664795}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 7, train_loss: 3.6160: 100%|███████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  8.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train Loss': 3.4650342300020416, 'Validation Metric': {'rmse': 2.0181937217712402}, 'Test Metric': {'rmse': 2.1566030979156494}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 8, train_loss: 1.7880: 100%|███████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  7.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train Loss': 3.273873941651706, 'Validation Metric': {'rmse': 1.9629745483398438}, 'Test Metric': {'rmse': 2.0976290702819824}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 9, train_loss: 3.5410: 100%|███████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  8.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train Loss': 2.9866720068043677, 'Validation Metric': {'rmse': 1.8636009693145752}, 'Test Metric': {'rmse': 1.9922807216644287}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 10, train_loss: 2.7221: 100%|██████████████████████████████████████████████████████████████| 29/29 [00:04<00:00,  6.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train Loss': 2.6042742811400315, 'Validation Metric': {'rmse': 1.8107463121414185}, 'Test Metric': {'rmse': 1.8894224166870117}}\n",
            "Best Validation Metric: 1.8107463121414185\n",
            "Test Metric: 1.8894224166870117\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training pipeline is general enough that we can train our Graphormer on numerous other tasks, including all that tasks that are listed in this [spreadsheet](https://github.com/snap-stanford/ogb/blob/master/ogb/graphproppred/master.csv); interpretations of these datasets can be found on page 15 of this [paper](https://arxiv.org/pdf/2307.04052.pdf).\n",
        "\n",
        "## Question 7 (15 points)\n",
        "\n",
        "Train your model on a toxicity dataset (Tox21) in a similar manner as above. The task here is to predict 12 binary labels for each molecule that correspond to its toxicity on 12 different targets. The metric used here is AUC. Adjust the hyperparameters of your model and/or training procedure to ensure that your test AUC is greater than 0.7. Plot the train loss and val/test metrics over epochs using the function defined above."
      ],
      "metadata": {
        "id": "pLKzdBxTle5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tox():\n",
        "    ## A function that trains the Graphormer on the toxicity dataset.\n",
        "    ############# Your code here ############\n",
        "    ## (~5-15 lines of code)\n",
        "\n",
        "    node_feature_num = 4\n",
        "    edge_feature_num = 2\n",
        "\n",
        "    model = Graphormer(\n",
        "        n_layers=3,\n",
        "        num_heads=5,\n",
        "        hidden_dim=64,\n",
        "        dropout_rate=0.1,\n",
        "        input_dropout_rate=0.1,\n",
        "        ffn_dim=128,\n",
        "        edge_type=\"multi_hop\",\n",
        "        multi_hop_max_dist=5,\n",
        "        attention_dropout_rate=0.1,\n",
        "        node_feature_num=node_feature_num,\n",
        "        edge_feature_num=edge_feature_num,\n",
        "    )\n",
        "    task_name = \"ogbg-moltox21\"\n",
        "\n",
        "    return train_model_on_task(\n",
        "        task_name,\n",
        "        model,\n",
        "        node_feature_num,\n",
        "        edge_feature_num,\n",
        "        emb_dim=64,\n",
        "        lr_model=1e-5,\n",
        "        lr_dec=1e-5,\n",
        "    )\n",
        "\n",
        "    #########################################\n",
        "\n",
        "val_metric, test_metric, curves = tox()\n",
        "plot_curves(curves)\n"
      ],
      "metadata": {
        "id": "V95gNOZYlb3I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4441d964-af76-410c-9d71-d9088b205662"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ogb/graphproppred/dataset_pyg.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset x shape: torch.Size([145459, 9]) Dataset edge attr shape: torch.Size([302190, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch: 1, train_loss: nan: 100%|████████████████████████████████████████████████████████████████| 196/196 [00:54<00:00,  3.63it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-bd4179287188>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mplot_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-bd4179287188>\u001b[0m in \u001b[0;36mtox\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtask_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogbg-moltox21\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     return train_model_on_task(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-9c051c32aa00>\u001b[0m in \u001b[0;36mtrain_model_on_task\u001b[0;34m(task_name, model, node_feature_num, edge_feature_num, num_epochs, emb_dim, batch_size, lr_model, lr_dec)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtrain_perf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mvalid_perf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mtest_perf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-b2f43b8b5f8c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model_list, device, loader, evaluator)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0minput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ogb/graphproppred/evaluate.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_metric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rocauc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_and_check_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_rocauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_metric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ap'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_and_check_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ogb/graphproppred/evaluate.py\u001b[0m in \u001b[0;36m_eval_rocauc\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;31m# ignore nan values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mis_labeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mrocauc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_labeled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_labeled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrocauc_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     if y_type == \"multiclass\" or (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
          ]
        }
      ]
    }
  ]
}